{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MNIST_GAN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTTsAdnKQYPT","outputId":"6d210374-462e-4148-b853-8c838424ac18"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from keras.datasets import mnist\n","from keras.models import Sequential,Model\n","from keras.layers import Dense,LeakyReLU,Input\n","from keras.optimizers import Adam\n","\n","##Initialisation\n","batch_size = 256\n","step_per_epoch = 3750\n","epochs = 20\n","\n","##Load the dataset\n","(x_train,x_test),(y_train,y_test) = mnist.load_data()\n","## Convert to float and Rescale -1 to 1 (Can also do 0 to 1)\n","x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n","x_train = x_train.reshape(-1,28*28*1)\n","\n","\n","\n","##Define standalone generator network \n","def Generator():\n","  generator = Sequential()\n","  generator.add(Dense(210,input_dim=100))\n","  generator.add(LeakyReLU(0.2))\n","\n","  generator.add(Dense(510))\n","  generator.add(LeakyReLU(0.2))\n","\n","  generator.add(Dense(810))\n","  generator.add(LeakyReLU(0.2))\n","\n","  generator.add(Dense(1010))\n","  generator.add(LeakyReLU(0.2))\n","\n","  generator.add(Dense(28*28*1,activation=\"tanh\"))\n","\n","  generator.compile(optimizer=Adam(0.0002,0.5),loss=\"binary_crossentropy\")\n","\n","  return generator\n","\n","##Define standalone Discriminator network    \n","def Descriminator():\n","  descriminator = Sequential()\n","  descriminator.add(Dense(1010,input_dim=28*28*1))\n","  descriminator.add(LeakyReLU(0.2))\n","\n","  descriminator.add(Dense(810))\n","  descriminator.add(LeakyReLU(0.2))\n","\n","  descriminator.add(Dense(510))\n","  descriminator.add(LeakyReLU(0.2))\n","\n","  descriminator.add(Dense(210))\n","  descriminator.add(LeakyReLU(0.2))\n","\n","  descriminator.add(Dense(1,activation=\"sigmoid\"))\n","  descriminator.compile(optimizer=Adam(0.0002,0.5),loss=\"binary_crossentropy\")\n","  return descriminator\n","\n","## Define the combined generator and discriminator model, for updating the generator \n","##This builds the discriminator  \n","desc = Descriminator()\n","##This builds the Generator\n","gen = Generator()\n","##This ensures that when we combine our networks we only train the Generator.\n","##While generator training we do not want discriminator weights to be adjusted. \n","##This Doesn't affect the above descriminator training.\n","desc.trainable = False\n","##Our random input \n","gan_input = Input(shape=(100,))\n","\n","##In a GAN the Generator network takes an input to produce its images\n","fake_img = gen(gan_input)\n","\n","##This specifies that Discriminator will take the images generated by our Generator  \n","gan_output = desc(fake_img)\n","\n","##The combined model of GAN with  generator and discriminator inputs\n","gan = Model(gan_input,gan_output)\n","## compile model of GAN\n","gan.compile(loss=\"binary_crossentropy\",optimizer=Adam(0.0002,0.5))\n","\n","## Manually enumerate epoch\n","for epoch in range(epochs):\n","## Enumerate batches over the training set  \n","  for batch in range(step_per_epoch):\n","##Select a random batch of images\n","    noise = np.random.normal(0,1,size=(batch_size,100))\n","## Generate a batch of fake images\n","    fake = gen.predict(noise)\n","## Train the discriminator on real and fake images\n","    real = x_train[np.random.randint(0,x_train.shape[0],size=batch_size)]\n","    x = np.concatenate((real,fake))\n","    label_real = np.ones(2*batch_size)\n","    label_real[:batch_size] = 0.9\n","##Take average loss from real and fake images for discriminator\n","    desc_loss = desc.train_on_batch(x,label_real)\n","##Take average loss from real and fake images for generator \n","    label_fake = np.zeros(batch_size)\n","    gen_loss = gan.train_on_batch(noise,label_fake)\n","\n","## In order to keep track of our training process, we print the\n","##progress\n","  print(f\"Epoch : {epoch} / Descriminator Loss : {desc_loss} / Generator Loss : {gen_loss}\")\n","\n","\n","##This function saves our images for us to view\n","def show_image(noise):\n"," images = gen.predict(noise)\n"," plt.figure(figsize=(5,4))\n"," for i,image in enumerate(images):\n","  plt.subplot(5,4,i+1)\n","  plt.imshow(images[i].reshape((28,28)))\n"," plt.show()\n","noise = np.random.normal(0,1,size=(20,100))\n","show_image(noise)\n","\n","\n"," \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Epoch : 0 / Descriminator Loss : 0.18195068836212158 / Generator Loss : 4.609687805175781\n","Epoch : 1 / Descriminator Loss : 0.18475976586341858 / Generator Loss : 4.2197980880737305\n","Epoch : 2 / Descriminator Loss : 0.18840208649635315 / Generator Loss : 3.851132869720459\n"],"name":"stdout"}]}]}